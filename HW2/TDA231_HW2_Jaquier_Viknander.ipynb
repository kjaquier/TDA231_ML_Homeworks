{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\qquad$ $\\qquad$$\\qquad$  **TDA 231 Machine Learning: Homework 2** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$ **Goal: Classification**<br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   **Grader: Divya** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                     **Due Date: 23/4** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   **Submitted by: Sandra Viknander, 9003012482, danvik@student.chalmers.se ; Kevin Jaquier, 921119T334, jaquier@student.chalmer.se** <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General guidelines:\n",
    "* All solutions to theoretical problems, can be submitted as a single file named *report.pdf*. They can also be submitted in this ipynb notebook, but equations wherever required, should be formatted using LaTeX math-mode.\n",
    "* All discussion regarding practical problems, along with solutions and plots should be specified here itself. We will not generate the solutions/plots again by running your code.\n",
    "* Your name, personal number and email address should be specified above and also in your file *report.pdf*.\n",
    "* All datasets can be downloaded from the course website.\n",
    "* All tables and other additional information should be included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical problems\n",
    "\n",
    "## [Naive Bayes Classifier, 6 points]\n",
    "\n",
    "A psychologist does a small survey on ''happiness''. Each respondent provides a vector with entries 1 or 0 corresponding to if they answered “yes” or “no” to a question respectively. The question vector has attributes \n",
    "$$\n",
    "x = (\\mbox{rich, married, healthy}) \\tag{1}\n",
    "$$\n",
    "\n",
    "Thus a response $(1, 0, 1)$ would indicate that the respondent was\n",
    "''rich'', ''unmarried'' and ''healthy''. In addition, each respondent\n",
    "gives a value $c = 1$ if they are content wih their life and $c = 0$\n",
    "if they’re not. The following responses were obtained.\n",
    "\n",
    "$$\n",
    "c = 1: (1, 1, 1),(0, 0, 1),(1, 1, 0),(1, 0, 1) \\\\\n",
    "c = 0: (0, 0, 0),(1, 0, 0),(0, 0, 1),(0, 1, 0)\n",
    "$$\n",
    "\n",
    "1. Using naive Bayes, what is the probability that a person is ''not rich'', ''married'' and ''healthy'' is ''content''?\n",
    "\n",
    "2. What is the probability that a person who is ''not rich'' and ''married'' is content ? (i.e. we do not know if they are ''healthy'')\n",
    "\n",
    "## [Extending Naive Bayes, 4 points]\n",
    "\n",
    "Consider now, the following vector of attributes:\n",
    "\n",
    "* $x_1 = 1$ if customer is younger than 20 and 0 otherwise.\n",
    "* $x_2 = 1$ if customer is between 20 and 30 in age, and 0 otherwise.\n",
    "* $x_3 = 1$ if customer is older than 30 and 0 otherwise\n",
    "* $x_4 = 1$ if customer walks to work and 0 otherwise.\n",
    "\n",
    "Each vector of attributes has a label ''rich'' or ''poor''. Point out potential difficulties with your approach above to training using naive Bayes. Suggest and describe how to extend your naive Bayes method to this dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions\n",
    "\n",
    "See report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical problems\n",
    "\n",
    "## [Bayes classifier, 5 points]\n",
    "\n",
    "Dowload the dataset **\"dataset2.txt\"**. You can use the following code for example:\n",
    "```python\n",
    "from numpy import genfromtxt\n",
    "data = genfromtxt('dataset2.txt', delimiter=',')\n",
    "labels = data[:,-1]\n",
    "```\n",
    "The dataset contains $3$-dimensional data, $X$, generated from $2$ classes with labels, $y$ either $+1$ or $-1$.  Each row of $X$ and $y$ contain one observation and one label respectively.  There are $1000$ instances of each class. \n",
    "\n",
    "a. Assume that the class conditional density is spherical Gaussian, and both classes have equal prior. Write the expression for the Bayes (<span style=\"color:red\"> not **naive Bayes**</span>) classifier i.e. derive\n",
    "$$\n",
    "P(y_{new} = -1 | x_{new} , X, y ) \\\\\n",
    "P(y_{new} = +1 | x_{new} , X, y ) ~.\n",
    "$$\n",
    "\n",
    "It is useful to note that the dependence on training data $X, y$ for class $1$ can be expressed as: \n",
    "\n",
    "$$ \n",
    "P( x_{new} | y_{new} = 1, X, y) = P(x_{new} |\n",
    "\\hat{\\mu}_{1}, \\hat{\\sigma}^{2}_{1})\n",
    "$$\n",
    "\n",
    "where $\\hat{\\mu}_{1} \\in \\mathbb{R}^3$ and $\\hat{\\sigma}^{2}_{1}\\in \\mathbb{R}$ are MLE estimates for mean (3-dimensional) and variance based on training data with label $+1$ (and similarly for class 2 with label $-1$). \n",
    "\n",
    "b. Implement a function **sph_bayes()** which computes the probability of a new test point *Xtest* coming from class $1$ ($P1$) and class $2$ ($P2$). Finally, assign a label *Ytest* to the test point based on the probabilities $P1$ and $P2$.\n",
    "\n",
    "```python\n",
    "def sph_bayes(Xtest, ...): # other parameters needed.\n",
    "\n",
    "    return [P1, P2, Ytest]\n",
    "```\n",
    "c. Write a function **new_classifier()**\n",
    "\n",
    "```python\n",
    "def new_classifier(Xtest, mu1, mu2)\n",
    "    \n",
    "    return [Ytest]\n",
    "```\n",
    "which implements the following classifier,\n",
    "$$\n",
    "f(x) = \\mbox{sign}\\left(\\frac{(\\mu_1 - \\mu_2)^\\top (x - b) }{\\|\\mu_1 -  \\mu_2\\|_2} \\right)\n",
    "$$\n",
    "with $b = \\frac{1}{2}(\\mu_1 + \\mu_2)$.\n",
    "\n",
    "d. Report 5-fold cross validation error for both classifiers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "a. See report\n",
    "\n",
    "b-d. See below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv, det, norm\n",
    "from scipy.stats import multivariate_normal\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "def sph_bayes(Xtest, mu1, mu2, var1, var2): # other parameters needed.\n",
    "    dist1 = multivariate_normal(mean=mu1, cov=var1)\n",
    "    dist2 = multivariate_normal(mean=mu2, cov=var2)\n",
    "    \n",
    "    lik1 = dist1.pdf(Xtest)\n",
    "    lik2 = dist2.pdf(Xtest)\n",
    "    \n",
    "    marg_lik = lik1+lik2\n",
    "    \n",
    "    P1 = (lik1 / marg_lik)\n",
    "    P2 = (lik2 / marg_lik)\n",
    "    \n",
    "    Ytest = (P1 > P2).astype(int) * 2.0 - 1.0\n",
    "\n",
    "    return [P1, P2, Ytest]\n",
    "\n",
    "\n",
    "def new_classifier(Xtest, mu1, mu2):\n",
    "    Ytest = np.empty(Xtest.shape[0])\n",
    "    b = (mu1 + mu2) / 2.0\n",
    "    mu_diff = mu1 - mu2\n",
    "    \n",
    "    for i in range(Ytest.shape[0]):\n",
    "        b_diff = Xtest[i,:] - b\n",
    "        mu_dist = np.sqrt(np.sum(np.square(mu_diff)))\n",
    "        Ytest[i] = np.dot(mu_diff, b_diff) / mu_dist\n",
    "    \n",
    "    return np.sign(Ytest)\n",
    "\n",
    "\n",
    "def get_gauss_params(data):\n",
    "    mu = np.mean(data, axis=0)\n",
    "    var = np.var(data)\n",
    "    return mu, var\n",
    "    \n",
    "    \n",
    "CVResult = namedtuple('CVResult', ('err_counts', 'test_samples_counts', 'err_ratios'))\n",
    "\n",
    "def cross_validate(data, labels, k, classifier):\n",
    "    data1 = data[labels > 0,:]\n",
    "    data2 = data[labels < 0,:]\n",
    "    \n",
    "    assert data1.shape[0] + data2.shape[0] == data.shape[0], \"bad labels\"\n",
    "    assert data1.shape[0] > 0, \"no sample for class 1\"\n",
    "    assert data2.shape[0] > 0, \"no sample for class 2\"\n",
    "    assert np.unique(labels).shape[0] == 2, \"more than two distinct labels\"\n",
    "    \n",
    "    prior_mu1, prior_var1 = get_gauss_params(data1)\n",
    "    prior_mu2, prior_var2 = get_gauss_params(data2)\n",
    "    \n",
    "    kf = KFold(n_splits=k)\n",
    "    err_counts = []\n",
    "    test_samples_counts = []\n",
    "    err_ratios = []\n",
    "\n",
    "    for train_index, test_index in kf.split(data):\n",
    "        X_train, X_test = data[train_index], data[test_index]\n",
    "        labels_train, labels_test = labels[train_index], labels[test_index]\n",
    "        \n",
    "        Y_train = classifier(X_train, prior_mu1, prior_mu2, prior_var1, prior_var2)\n",
    "        post_mu1, post_var1 = get_gauss_params(X_train[Y_train > 0.0,:])\n",
    "        post_mu2, post_var2 = get_gauss_params(X_train[Y_train < 0.0,:])\n",
    "        Y_test = classifier(X_test, post_mu1, post_mu2, post_var1, post_var2)\n",
    "\n",
    "        err_count = np.sum(~np.isclose(Y_test, labels_test))\n",
    "        err_counts.append(err_count)\n",
    "        \n",
    "        n = X_test.shape[0]\n",
    "        test_samples_counts.append(n)\n",
    "    \n",
    "    err_counts = np.array(err_counts)\n",
    "    test_samples_counts = np.array(test_samples_counts)\n",
    "        \n",
    "    return CVResult(\n",
    "        err_counts=err_counts,\n",
    "        test_samples_counts=test_samples_counts,\n",
    "        err_ratios=np.divide(err_counts, test_samples_counts),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification error rates per fold [5-fold]\n",
      "Spherical Bayes:   [ 0.  0.  0.  0.  0.]\n",
      "\"New classifier\":  [ 0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "data = np.genfromtxt('dataset2.txt', delimiter=',')\n",
    "labels = data[:,-1]\n",
    "data = data[:,:3]\n",
    "\n",
    "K = 5\n",
    "\n",
    "res_sb = cross_validate(\n",
    "    data=data,\n",
    "    labels=labels,\n",
    "    k=K,\n",
    "    classifier=lambda X, mu1, mu2, var1, var2: sph_bayes(X, mu1, mu2, var1, var2)[2]\n",
    ")\n",
    "\n",
    "res_nc = cross_validate(\n",
    "    data=data,\n",
    "    labels=labels,\n",
    "    k=K,\n",
    "    classifier=lambda X, mu1, mu2, _var1, _var2: new_classifier(X, mu1, mu2)\n",
    ")\n",
    "\n",
    "print(f\"Classification error rates per fold [{K}-fold]\")\n",
    "print(\"Spherical Bayes:  \", res_sb.err_ratios)\n",
    "print('\"New classifier\": ', res_sb.err_ratios)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## [DIGITS dataset classifer, 5 points]\n",
    "\n",
    "Load the DIGITS dataset:\n",
    "```python\n",
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "```\n",
    "This dataset contains $1797$ samples of ten handwritten digit classes. You can further query and visualize the dataset using the various attributes of the returned dictionary:\n",
    "```python\n",
    "data = digits.data\n",
    "print(data.shape)\n",
    "target_names = digits.target_names\n",
    "print (target_names)\n",
    "import matplotlib.pyplot as plt\n",
    "y = digits.target\n",
    "plt.matshow(digits.images[0])\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "a. Use **new_classifier()** designed previously to do binary classification between classes representing digits \"*5*\" and \"*8*\".\n",
    "\n",
    "b. Investigate an alternative feature function as described below:\n",
    "\n",
    "1. Scale each pixel value to range $[0, 1] $ from original gray-scale ($0-255$). \n",
    "2. Compute variance of each row and column of the image. This will give you a new feature vector of size $16$ i.e. \n",
    "\n",
    "$$ \n",
    "x' = \\left[ \\; Var(row_1)  , Var(row_2), \\ldots , Var(row_{8}), Var(col_1), \\ldots, Var(col_{8}) \\;\\right]^T\n",
    "$$\n",
    "\n",
    "c. Report $5$-fold cross validation results for parts $(a)$ and\n",
    "$(b)$ in a single table. What can you say about the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification error rates per fold [5-fold]\n",
      "From pixels:             [ 0.01388889  0.01408451  0.          0.05633803  0.02816901]\n",
      "From extracted features: [ 0.25        0.15492958  0.12676056  0.12676056  0.16901408]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "data = digits.data\n",
    "\n",
    "Y = digits.target\n",
    "subset = np.array([y in [5,8] for y in Y])\n",
    "X = digits.data[subset, :]\n",
    "labels = np.array([+1 if y == 5 else -1 for y in Y[subset]])\n",
    "\n",
    "K = 5\n",
    "\n",
    "res_a = cross_validate(\n",
    "    data=X,\n",
    "    labels=labels,\n",
    "    k=K,\n",
    "    classifier=lambda X, mu1, mu2, _var1, _var2: new_classifier(X, mu1, mu2)\n",
    ")\n",
    "\n",
    "def extract_features(X):\n",
    "    X_features = []\n",
    "    for i in range(X.shape[0]):\n",
    "        x = X[i,:].reshape((8, 8))\n",
    "        var_cols = np.var(x, axis=0)\n",
    "        var_rows = np.var(x, axis=1)\n",
    "        features = np.concatenate([var_rows, var_cols])\n",
    "        \n",
    "        X_features.append(features)\n",
    "    \n",
    "    return np.array(X_features)\n",
    "\n",
    "assert np.min(X) >= 0.0\n",
    "X_scaled = X / np.max(X)\n",
    "X_prime = extract_features(X_scaled)\n",
    "\n",
    "res_b = cross_validate(\n",
    "    data=X_prime,\n",
    "    labels=labels,\n",
    "    k=K,\n",
    "    classifier=lambda X, mu1, mu2, _var1, _var2: new_classifier(X, mu1, mu2)\n",
    ")\n",
    "\n",
    "print(f\"Classification error rates per fold [{K}-fold]\")\n",
    "print(\"From pixels:            \", res_a.err_ratios)\n",
    "print(\"From extracted features:\", res_b.err_ratios)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As we can see with the error rates above, this classifier performs better using the pixels directly than using the variance of each rows and columns, even though the dimensionality is higher in this case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
